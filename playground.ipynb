{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07c94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24be481",
   "metadata": {},
   "source": [
    "### QWEN 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4bbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55246777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defeab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.utils import load_extractions\n",
    "\n",
    "folder = \"extractions/Qwen_Qwen2.5-VL-3B-Instruct_VSR\"\n",
    "\n",
    "data = load_extractions(folder, keys=[\"attns\", \"image_positions\", \"text_positions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab6bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn, image_positions, text_positions = data['attns'], data['image_positions'], data['text_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d441770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.attention import compute_attention_ratios, compute_attention_ratios2\n",
    "\n",
    "im1, _ = compute_attention_ratios2(attn, image_positions)\n",
    "im2, _ = compute_attention_ratios(attn, image_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ade54281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 19.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from typing import List, Dict\n",
    "\n",
    "MIN_PIXELS = 250 * 28 * 28\n",
    "MAX_PIXELS = 256 * 28 * 28\n",
    "MAX_SEQ_LEN = 300\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "            model_name,\n",
    "            min_pixels=MIN_PIXELS,\n",
    "            max_pixels=MAX_PIXELS,\n",
    "            padding_side=\"left\",\n",
    "        )\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "model_name,\n",
    "torch_dtype=torch.bfloat16,\n",
    "attn_implementation=\"flash_attention_2\" if eval else \"eager\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df4e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e23e8fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extractions/Qwen_Qwen2.5-VL-3B-Instruct_VSR/processor'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"extractions/Qwen_Qwen2.5-VL-3B-Instruct_VSR/\"\n",
    "processor_output_dir = os.path.join(save_dir, \"processor\")\n",
    "processor_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d95ad3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(processor_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "057d9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model.config.to_json_file(os.path.join(\"\", \"model_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e6341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_dataset\n",
    "\n",
    "dataset = get_dataset(\"VSR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[100]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899e6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "MAX_PIXELS = 256 * 28 * 28\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", max_pixels = MAX_PIXELS)\n",
    "\n",
    "#image = \"https://picsum.photos/512\"\n",
    "image = sample['image_options'][0]\n",
    "prompt = \"Give me three facts about this image\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": prompt},\n",
    "        ],\n",
    "    },\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "image_inputs, _ = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=None,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "inputs.image_grid_thw, inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ceeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=None,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "inputs.image_grid_thw, inputs.pixel_values.shape, (inputs.input_ids == 151655).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6a32ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = sample['image_options'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_w = 2300\n",
    "orig_h = 4096\n",
    "\n",
    "patch  = processor.image_processor.patch_size     # 14 px\n",
    "merge  = processor.image_processor.merge_size     # 2 by default\n",
    "print(patch, merge)\n",
    "\n",
    "_, H_grid, W_grid = inputs.image_grid_thw[0].tolist()  # e.g. 42×22\n",
    "\n",
    "H_tok, W_tok = H_grid // merge, W_grid // merge        # 21×11 → 231\n",
    "\n",
    "H_real = H_grid * patch                           # 588\n",
    "W_real = W_grid * patch                           # 308\n",
    "\n",
    "sx = orig_w / W_real\n",
    "sy = orig_h / H_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4341f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx, sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef64cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, y0, x1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72361279",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for k in range(231):   # k = 0..230\n",
    "    row, col = divmod(k, W_tok)               # token’s row/col in merged grid\n",
    "\n",
    "    # top-left raw patch of this merged token\n",
    "    r0_p, c0_p = row * merge, col * merge\n",
    "\n",
    "    # bounding box in **processed** coords\n",
    "    x0_p = c0_p * patch\n",
    "    y0_p = r0_p * patch\n",
    "    x1_p = (c0_p + merge) * patch\n",
    "    y1_p = (r0_p + merge) * patch\n",
    "\n",
    "    # scale to original resolution\n",
    "    x0 = int(round(x0_p * sx));  y0 = int(round(y0_p * sy))\n",
    "    x1 = int(round(x1_p * sx));  y1 = int(round(y1_p * sy))\n",
    "\n",
    "    draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4.  Display\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605429c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    ").cuda()\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "image = \"https://picsum.photos/512\"\n",
    "prompt = \"Give me three facts about this image\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": prompt},\n",
    "        ],\n",
    "    },\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "batch_messages = [messages]\n",
    "text_inputs = [\n",
    "    processor.apply_chat_template(\n",
    "            msgs, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        for msgs in batch_messages\n",
    "]\n",
    "# Preparation for inference\n",
    "# text = processor.apply_chat_template(\n",
    "#     messages, tokenize=False, add_generation_prompt=True\n",
    "# )\n",
    "image_inputs, video_inputs = process_vision_info(batch_messages)\n",
    "inputs = processor(\n",
    "    text=text_inputs,\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f07d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a76e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-VL-asd\"\n",
    "key = model_name.lower().split(\"/\")[1]\n",
    "\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb9d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e72938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569764b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "298a5339",
   "metadata": {},
   "source": [
    "### InternVL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe8fe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set `load_in_8bit=True`, you will need two 80GB GPUs.\n",
    "# If you set `load_in_8bit=False`, you will need at least three 80GB GPUs.\n",
    "path = 'OpenGVLab/InternVL3-1B'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=False,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('example.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# single-image single-round conversation (单图单轮对话)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "path = \"OpenGVLab/InternVL3-1B\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "pixel_values = load_image(\"example.jpg\", max_num=12).to(torch.bfloat16).cuda()\n",
    "\n",
    "question = \"<image>\\nPlease describe the image shortly.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "#inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "# Attach the image tensors under the keyword the model expects\n",
    "# (here we assume it’s just “pixel_values”)\n",
    "inputs[\"pixel_values\"] = pixel_values\n",
    "\n",
    "# 4) Configure generation parameters\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    generation_config=gen_config,\n",
    ")\n",
    "\n",
    "full_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "if full_output.startswith(question):\n",
    "    response = full_output[len(question):].strip()\n",
    "else:\n",
    "    response = full_output.strip()\n",
    "\n",
    "print(f\"User: {question}\\nAssistant: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
